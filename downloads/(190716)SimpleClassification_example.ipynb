{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Classification 구현\n",
    "### 주의해서 볼 함수는 sigmoid,  loss_func,  predict 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_data.shape =  (10, 1) , t_data.shape =  (10, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "x_data = np.array([2, 4, 6, 8, 10, 12, 14, 16, 18, 20]).reshape(10,1)   \n",
    "t_data = np.array([0, 0, 0, 0,  0,  0,  1,  1,  1,  1]).reshape(10,1)\n",
    "\n",
    "print(\"x_data.shape = \", x_data.shape, \", t_data.shape = \", t_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W =  [[0.68324175]] , W.shape =  (1, 1) , b =  [0.16782983] , b.shape =  (1,)\n"
     ]
    }
   ],
   "source": [
    "W = np.random.rand(1,1)  \n",
    "b = np.random.rand(1)  \n",
    "print(\"W = \", W, \", W.shape = \", W.shape, \", b = \", b, \", b.shape = \", b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최종출력은 y = sigmoid(Wx+b) 이며, 손실함수는 cross-entropy 로 나타냄\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1+np.exp(-z))\n",
    "\n",
    "def loss_func(x, t):\n",
    "    \n",
    "    delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "    z = np.dot(x, W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    # cross-entropy \n",
    "    return  -np.sum( t*np.log(y + delta) + (1-t)*np.log((1 - y)+delta ) ) \n",
    "\n",
    "# 손실함수 값 계산 함수\n",
    "# 입력변수 x, t : numpy type\n",
    "def error_val(x, t):\n",
    "    delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "    z = np.dot(x, W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    # cross-entropy \n",
    "    return  -np.sum( t*np.log(y + delta) + (1-t)*np.log((1 - y)+delta ) ) \n",
    "\n",
    "# 학습을 마친 후, 임의의 데이터에 대해 미래 값 예측 함수\n",
    "# 입력변수 test_data : numpy type\n",
    "def predict(test_data):\n",
    "    \n",
    "    z = np.dot(test_data, W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    if y >= 0.5:\n",
    "        result = 1  # True\n",
    "    else:\n",
    "        result = 0  # False\n",
    "    \n",
    "    return y, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error value =  29.970030505605433 Initial W =  [[0.68324175]] \n",
      " , b =  [0.16782983]\n",
      "step =  0 error value =  13.125022665994706 W =  [[0.27018403]] , b =  [0.11693771]\n",
      "step =  5000 error value =  0.8442884510985377 W =  [[0.90056376]] , b =  [-11.55610192]\n",
      "step =  10000 error value =  0.6206069108544321 W =  [[1.15346512]] , b =  [-14.87057485]\n",
      "step =  15000 error value =  0.5092682272156452 W =  [[1.3333458]] , b =  [-17.22116772]\n",
      "step =  20000 error value =  0.4376282318713728 W =  [[1.47800144]] , b =  [-19.10881978]\n",
      "step =  25000 error value =  0.38603387227718144 W =  [[1.60090794]] , b =  [-20.71133951]\n",
      "step =  30000 error value =  0.34643713744927196 W =  [[1.70865284]] , b =  [-22.11541155]\n",
      "step =  35000 error value =  0.31477847024999833 W =  [[1.80503547]] , b =  [-23.3709353]\n",
      "step =  40000 error value =  0.2887303157589254 W =  [[1.89248682]] , b =  [-24.50979249]\n",
      "step =  45000 error value =  0.26683694459460516 W =  [[1.97267742]] , b =  [-25.55386699]\n",
      "step =  50000 error value =  0.24812890339250349 W =  [[2.04681646]] , b =  [-26.51898413]\n",
      "step =  55000 error value =  0.23192927135469948 W =  [[2.11581421]] , b =  [-27.41704854]\n",
      "step =  60000 error value =  0.21774761341733215 W =  [[2.18037728]] , b =  [-28.25729532]\n",
      "step =  65000 error value =  0.20521794762128096 W =  [[2.24106774]] , b =  [-29.04706689]\n",
      "step =  70000 error value =  0.19406047969854354 W =  [[2.29834174]] , b =  [-29.79231902]\n",
      "step =  75000 error value =  0.1840569521587267 W =  [[2.35257562]] , b =  [-30.49796365]\n",
      "step =  80000 error value =  0.17503418086983888 W =  [[2.40408424]] , b =  [-31.16810895]\n",
      "step =  85000 error value =  0.16685272025600825 W =  [[2.45313416]] , b =  [-31.80623206]\n",
      "step =  90000 error value =  0.15939885418824679 W =  [[2.4999534]] , b =  [-32.41530656]\n",
      "step =  95000 error value =  0.1525788085396157 W =  [[2.54473879]] , b =  [-32.99789832]\n",
      "step =  100000 error value =  0.14631448666434718 W =  [[2.5876615]] , b =  [-33.556239]\n",
      "step =  105000 error value =  0.14054027266308558 W =  [[2.62887154]] , b =  [-34.09228332]\n",
      "step =  110000 error value =  0.13520059837275533 W =  [[2.66850111]] , b =  [-34.60775413]\n",
      "step =  115000 error value =  0.13024806634949454 W =  [[2.70666742]] , b =  [-35.1041786]\n",
      "step =  120000 error value =  0.1256419840629845 W =  [[2.74347491]] , b =  [-35.58291737]\n",
      "step =  125000 error value =  0.12134720657358397 W =  [[2.7790171]] , b =  [-36.04518838]\n",
      "step =  130000 error value =  0.11733321361715093 W =  [[2.81337803]] , b =  [-36.4920865]\n",
      "step =  135000 error value =  0.11357336689934665 W =  [[2.84663358]] , b =  [-36.92459984]\n",
      "step =  140000 error value =  0.1100443074132754 W =  [[2.87885248]] , b =  [-37.34362337]\n",
      "step =  145000 error value =  0.10672546262188976 W =  [[2.91009719]] , b =  [-37.74997047]\n",
      "step =  150000 error value =  0.10359864061758932 W =  [[2.94042465]] , b =  [-38.1443826]\n",
      "step =  155000 error value =  0.10064769371183403 W =  [[2.96988695]] , b =  [-38.52753772]\n",
      "step =  160000 error value =  0.09785823787353673 W =  [[2.99853181]] , b =  [-38.90005733]\n",
      "step =  165000 error value =  0.09521741741284638 W =  [[3.02640313]] , b =  [-39.26251267]\n",
      "step =  170000 error value =  0.09271370656353235 W =  [[3.05354132]] , b =  [-39.61543001]\n",
      "step =  175000 error value =  0.09033674134429269 W =  [[3.07998372]] , b =  [-39.95929531]\n",
      "step =  180000 error value =  0.08807717641123754 W =  [[3.10576489]] , b =  [-40.29455826]\n",
      "step =  185000 error value =  0.08592656264999025 W =  [[3.13091685]] , b =  [-40.62163579]\n",
      "step =  190000 error value =  0.08387724206751834 W =  [[3.15546937]] , b =  [-40.94091524]\n",
      "step =  195000 error value =  0.08192225718417846 W =  [[3.17945014]] , b =  [-41.25275706]\n",
      "step =  200000 error value =  0.08005527263516231 W =  [[3.20288498]] , b =  [-41.5574973]\n",
      "step =  205000 error value =  0.07827050709684162 W =  [[3.22579801]] , b =  [-41.85544974]\n",
      "step =  210000 error value =  0.07656267398029458 W =  [[3.24821177]] , b =  [-42.14690781]\n",
      "step =  215000 error value =  0.07492692959816423 W =  [[3.27014739]] , b =  [-42.43214636]\n",
      "step =  220000 error value =  0.07335882772569001 W =  [[3.29162468]] , b =  [-42.71142318]\n",
      "step =  225000 error value =  0.07185427965172 W =  [[3.31266224]] , b =  [-42.98498036]\n",
      "step =  230000 error value =  0.0704095189592699 W =  [[3.33327757]] , b =  [-43.25304558]\n",
      "step =  235000 error value =  0.06902107039365943 W =  [[3.35348715]] , b =  [-43.51583323]\n",
      "step =  240000 error value =  0.06768572227413008 W =  [[3.3733065]] , b =  [-43.77354538]\n",
      "step =  245000 error value =  0.0664005019862955 W =  [[3.3927503]] , b =  [-44.02637278]\n",
      "step =  250000 error value =  0.06516265416084009 W =  [[3.41183238]] , b =  [-44.27449562]\n",
      "step =  255000 error value =  0.06396962120049596 W =  [[3.43056586]] , b =  [-44.51808434]\n",
      "step =  260000 error value =  0.06281902586539068 W =  [[3.44896312]] , b =  [-44.7573003]\n",
      "step =  265000 error value =  0.06170865566704367 W =  [[3.46703594]] , b =  [-44.99229645]\n",
      "step =  270000 error value =  0.06063644885522249 W =  [[3.48479545]] , b =  [-45.22321784]\n",
      "step =  275000 error value =  0.05960048181125136 W =  [[3.50225224]] , b =  [-45.4502022]\n",
      "step =  280000 error value =  0.058598957685422136 W =  [[3.51941639]] , b =  [-45.67338044]\n",
      "step =  285000 error value =  0.057630196137644696 W =  [[3.53629745]] , b =  [-45.89287704]\n",
      "step =  290000 error value =  0.0566926240582461 W =  [[3.55290454]] , b =  [-46.1088105]\n",
      "step =  295000 error value =  0.055784767161342425 W =  [[3.56924633]] , b =  [-46.32129372]\n",
      "step =  300000 error value =  0.05490524235648617 W =  [[3.5853311]] , b =  [-46.53043431]\n",
      "step =  305000 error value =  0.054052750815813254 W =  [[3.60116674]] , b =  [-46.73633498]\n",
      "step =  310000 error value =  0.053226071663778116 W =  [[3.61676079]] , b =  [-46.93909375]\n",
      "step =  315000 error value =  0.052424056225231525 W =  [[3.63212045]] , b =  [-47.13880431]\n",
      "step =  320000 error value =  0.05164562277500788 W =  [[3.6472526]] , b =  [-47.33555622]\n",
      "step =  325000 error value =  0.050889751738776086 W =  [[3.66216384]] , b =  [-47.52943516]\n",
      "step =  330000 error value =  0.050155481300528416 W =  [[3.67686047]] , b =  [-47.72052316]\n",
      "step =  335000 error value =  0.04944190337706239 W =  [[3.69134853]] , b =  [-47.9088988]\n",
      "step =  340000 error value =  0.04874815992424255 W =  [[3.70563381]] , b =  [-48.09463741]\n",
      "step =  345000 error value =  0.04807343954347694 W =  [[3.71972186]] , b =  [-48.27781122]\n",
      "step =  350000 error value =  0.04741697436045219 W =  [[3.73361802]] , b =  [-48.45848955]\n",
      "step =  355000 error value =  0.04677803715089417 W =  [[3.7473274]] , b =  [-48.63673895]\n",
      "step =  360000 error value =  0.0461559386909052 W =  [[3.76085492]] , b =  [-48.81262337]\n",
      "step =  365000 error value =  0.04555002531164724 W =  [[3.7742053]] , b =  [-48.98620427]\n",
      "step =  370000 error value =  0.04495967664025685 W =  [[3.78738308]] , b =  [-49.15754074]\n",
      "step =  375000 error value =  0.044384303510625576 W =  [[3.80039264]] , b =  [-49.32668965]\n",
      "step =  380000 error value =  0.04382334602939002 W =  [[3.81323819]] , b =  [-49.49370575]\n",
      "step =  385000 error value =  0.043276271783787096 W =  [[3.82592378]] , b =  [-49.65864174]\n",
      "step =  390000 error value =  0.042742574179436714 W =  [[3.83845332]] , b =  [-49.82154842]\n",
      "step =  395000 error value =  0.042221770897177976 W =  [[3.85083057]] , b =  [-49.98247475]\n",
      "step =  400000 error value =  0.04171340245914949 W =  [[3.86305915]] , b =  [-50.14146796]\n",
      "\n",
      "Elapsed Time =>  0:00:37.029591\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-2  # 발산하는 경우, 1e-3 ~ 1e-6 등으로 바꾸어서 실행\n",
    "\n",
    "f = lambda x : loss_func(x_data,t_data)  # f(x) = loss_func(x_data, t_data)\n",
    "\n",
    "print(\"Initial error value = \", error_val(x_data, t_data), \"Initial W = \", W, \"\\n\", \", b = \", b )\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for step in  range(400001):  \n",
    "    \n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    \n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step % 5000 == 0):\n",
    "        print(\"step = \", step, \"error value = \", error_val(x_data, t_data), \"W = \", W, \", b = \",b )\n",
    "        \n",
    "end_time = datetime.now()\n",
    "        \n",
    "print(\"\")\n",
    "print(\"Elapsed Time => \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.69989367e-16] 0\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([3.7])\n",
    "\n",
    "(real_val, logical_val) = predict(test_data)\n",
    "\n",
    "print(real_val, logical_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.] 1\n"
     ]
    }
   ],
   "source": [
    "test_data = np.array([31.09])\n",
    "\n",
    "(real_val, logical_val) = predict(test_data)\n",
    "\n",
    "print(real_val, logical_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
